{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb65068-4fba-4a1a-ae71-1566dc4e7dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctg import CTG\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c1ba3-5f5b-4b43-acce-3e74e5b54584",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/scratch/alb9742/\"\n",
    "model_path = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "#model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "#model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, cache_dir=cache_dir, use_safetensors=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(\"DEBUG::GPU memory:: \", torch.cuda.memory_allocated(0))\n",
    "    model.cuda() # .to(model.device)\n",
    "    print(\"DEBUG::Model succesfully moved to Cuda.\")\n",
    "    print(\"DEBUG::GPU memory:: \", torch.cuda.memory_allocated(0))\n",
    "    CUDA = True\n",
    "except:\n",
    "    CUDA = False\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path, cache_dir=cache_dir, use_safetensors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a64b6f-de53-4396-8b5c-a388ffb7efd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CTG(model, tokenizer, mode=\"topk\", k=100, temperature=0.3, random_state=SEED cuda=CUDA)\n",
    "\n",
    "_, _ = m.generate(prompt=prompt, tau=1.0, max_tokens=250, embedder=embedder, clf=clf,verbose=\"True\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
